<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Workshop: Target Capture Data Processing Pipeline</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #2980b9;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 10px;
        }
        h3 {
            color: #34495e;
        }
        .code-block {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            border-left: 4px solid #e74c3c;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .important {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .tip {
            background-color: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        .warning {
            background-color: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .exercise {
            background-color: #e8f4fc;
            border: 2px dashed #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .caption {
            font-style: italic;
            color: #666;
            margin-top: 5px;
            font-size: 0.9em;
        }
        .param-detail {
            background-color: #ffffff;
            border: 1px solid #bdc3c7;
            padding: 10px;
            margin: 5px 0;
            border-radius: 3px;
            font-family: monospace;
        }
        .param-highlight {
            background-color: #e8f8f5;
            border-left: 3px solid #1abc9c;
            padding: 10px;
            margin: 10px 0;
        }
    </style>
</head>
<body>

<h1>üî¨ Target Capture Data Processing: A Modular Bash Pipeline</h1>
<p><strong>Objective:</strong> This workshop provides a structured, production-ready pipeline for processing target capture sequencing data. From raw FASTQ files to consensus sequences and phylogenies, each step is compartmentalized into functions for clarity, reproducibility, and scalability on HPC clusters.</p>

<div class="important">
    <strong>‚öôÔ∏è System Requirements:</strong> This pipeline is designed for a SLURM-based HPC environment. It assumes access to <code>conda</code>, <code>wget</code>, and the bioinformatics tools installed in the <code>target_capture</code> environment.
</div>

<h2>üìÅ Pipeline Architecture & Compartmentalization</h2>
<p>The pipeline is built as a series of independent Bash functions. This modular design allows you to run specific steps without re-running the entire workflow. Each function has a clear input/output interface, making the code easy to debug and maintain.</p>
<p>The entire script... almost</p>
<div class="code-block">    
    <pre>
#!/bin/bash
#SBATCH --job-name=my_array_job
#SBATCH --array=1       # Run tasks 1 through 10
#SBATCH --cpus-per-task=4     # Request 4 CPUs per task
#SBATCH --mem=4G            # we don't need much memory for this

function setup_conda {

conda create  --name target_capture
conda install --name target_capture -y fastp
conda install --name target_capture -y bwa
conda install --name target_capture -y samtools
conda install --name target_capture -y bcftools
conda install --name target_capture -y iqtree
conda install --name target_capture -y seqkit
conda install --name target_capture -y blast
}

function download_ena {

OUTDIR=$1

wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776786/ERR776786_1.fastq.gz
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776786/ERR776786_2.fastq.gz 
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776790/ERR776790_1.fastq.gz
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776790/ERR776790_2.fastq.gz

}

#CLEANING UP READS AND QC REPORT

function fastp_qc {

#SKIP TRIM ADAPTOR
#annotion 
INDIR=$1
OUTDIR=$2
SAMPLE=$3

fastp -i ${INDIR}/${SAMPLE}_1.fastq.gz -I ${INDIR}/${SAMPLE}_2.fastq.gz \
      -o ${OUTDIR}/${SAMPLE}_R1.fq.gz   -O ${OUTDIR}/${SAMPLE}_R2.fq.gz \
      --unpaired1 ${OUTDIR}/${SAMPLE}_unpaired1.fq.gz \
      --unpaired2 ${OUTDIR}/${SAMPLE}_unpaired2.fq.gz \
      -h ${OUTDIR}/${SAMPLE}.html \
      --json ${OUTDIR}/${SAMPLE}.json \
      --detect_adapter_for_pe \
      --qualified_quality_phred 20 \
      --unqualified_percent_limit 40\
      -c \
      --cut_front \
      --cut_front_mean_quality 20 \
      --cut_tail \
      --cut_front_mean_quality 20 \
      --cut_right \
      --cut_right_window_size 4 \
      --cut_right_mean_quality 20 \
      --length_required 36 \
      --trim_poly_g \
      --thread 8 \
      > ${OUTDIR}/${SAMPLE}.fastp.log

# Enable 5' trimming (LEADING equivalent)
# Quality threshold for 5' trim
# Enable 3' trimming (TRAILING equivalent)
# Quality threshold for 3' trim  
# Enable sliding window trim (SLIDINGWINDOW equivalent)
# Window size = 4
# Mean quality threshold = 15
# MINLEN equivalent
#--trim_poly_g                    force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data
}

#MAPPING
#BOWTIE2 MAPPING PIPELINE
function bowtie2_map {

INDIR=$1
IDX=$2
OUTDIR=$3
SAMPLE=$4

minQ=10

echo 'RUNNING BOWTIE2 MAPPING ON' $SAMPLE
bowtie2-align-s --wrapper basic-0 \
                -x $IDX \
                -1 ${INDIR}/${SAMPLE}_R1.fq.gz \
                -2 ${INDIR}/${SAMPLE}_R2.fq.gz \
                -p 20 \
                -N 1 \
                -L 20 \
                --threads 8 \
                --phred33 \
                --very-sensitive-local \
                --no-discordant \
                --no-mixed \
                --no-unal \
                --time \
                --rg-id ${SAMPLE} \
                --rg SM:${SAMPLE} \
                --rg PL:'ILLUMINA' |\
                samtools view -Sbh -F 4 -@ 8 -o $OUTDIR/${SAMPLE}.all.bam #all mapped reads
#number of all mapped reads
#samtools view -F 4 -c $OUTDIR/${prefix}_${SLURM_ARRAY_TASK_ID}.all.bam > $log

#only paired mapped reads with q>minQ
samtools view $OUTDIR/${SAMPLE}.all.bam -Sbh -F 4 -f 3 -q $minQ -@ 8 |samtools sort  -@ 8 -o $OUTDIR/${SAMPLE}.sorted.bam

echo 'RUNNING SAMTOOLS INDEXING ON' ${SAMPLE}
samtools index $OUTDIR/${SAMPLE}.sorted.bam
}

#BWA MAPPING PIPELINE
function bwa_map {

INDIR=$1
REF=$2
OUTDIR=$3
SAMPLE=$4

minQ=10

echo 'RUNNING BWA MAPPING ON' ${SAMPLE}
bwa mem $REF \
        ${INDIR}/${SAMPLE}_R1.fq.gz \
        ${INDIR}/${SAMPLE}_R2.fq.gz \
        -t 8 \
        -k 20 |\
        samtools view -Sbh -F 4 -@ 8 -o $OUTDIR/${SAMPLE}.all.bam

#only paired mapped reads with q>30
samtools view $OUTDIR/${SAMPLE}.all.bam -Sbh -F 4 -f 3 -q $minQ -@ 8 |samtools sort  -@ 8 -o $OUTDIR/${SAMPLE}.sorted.bam
#INDEX
echo 'RUNNING SAMTOOLS INDEXING ON' ${SAMPLE}
samtools index $OUTDIR/${SAMPLE}.sorted.bam
}

#SNP calling : 
function snp_calling {

INDIR=$1
OUTDIR=$2
REF=$3
SAMPLE=$4

minQ=20
minDP=10

echo 'calling SNPs'
#ls $INDIR/${prefix}*sorted.bam > bamlist.txt #need to change back to sorted.bam
#can ues a --bam-list $bamlist optio if you're doing all samples at the same time
bcftools mpileup -Ou -f $REF $INDIR/${SAMPLE}.sorted.bam --threads 8 \
                 --annotate INFO/AD,FORMAT/DP,FORMAT/AD \
 | bcftools call -Ou -mv \
 | bcftools filter -s LowQual -e "QUAL<${minQ} || INFO/DP<${minDP}" \
 > $OUTDIR/${SAMPLE}.flt1.vcf #combined depth across samples>100 or quality >20
#this is a soft filter, which does not remove any snps. you can remove them later in the df easily

}

#SNPS FILTERING
function bcftools_filter {

INDIR=$1
OUTDIR=$2
SAMPLE=$3

echo "Filtering diploid SNPs to tetraploid-like standards: $NAME"
    
     bcftools view $INDIR/${SAMPLE}.flt1.vcf -O u 2>/dev/null \
 |   bcftools view -i '1==1' 2>/dev/null \
 |   bcftools +fill-tags -- -t AC,AN,AF,MAF,HWE  \
 |   bcftools filter   -e "QUAL<30" \
 |   bcftools filter   -e 'INFO/MAF[0] < 0.01'  \
 |   bcftools filter   -e 'INFO/MQ < 30'  \
 |   bcftools filter   -e 'INFO/VDB < 0.1'  -Oz -o $OUTDIR/${SAMPLE}.flt3.vcf.gz
    
    echo "Indexing filtered VCF..."
    bcftools index $OUTDIR/${SAMPLE}.flt3.vcf.gz -f

# |   bcftools filter   -i 'INFO/AC[1] >= 3' \
# |   bcftools filter   -i '(COUNT(FMT/DP<5) + COUNT(FMT/DP="."))<= 2 && COUNT(FMT/DP>30) <= 1' \
}

#MAKE CONSENSUS FROM VCF
#THIS ONE NEEDS MORE MODIFICATION. WE CAN WORRY ABOUT IT LATER
function extract_consensus { 

INDIR=$1
OUTDIR=$2
REF=$3
SAMPLE=$4

echo "EXTRACT CONSENSUS SEQUENCE FROM $REF REF AND $SAMPLE"

#FOR LOOP OVER ALL THE GENES IN THE BAIT SET
for HEADING in $(grep '>' $REF)

do

  CHRM=${HEADING/>/};echo "Processing $CHRM ..."
  mkdir ${OUTDIR}/${CHRM}_tmp #a temporary directory so the files won't mix and interfere with each other
   
    # Generate consensus for this sample
    samtools faidx $REF $CHRM | bcftools consensus $INDIR/${SAMPLE}.flt3.vcf.gz -s $SAMPLE -I -o ${OUTDIR}/${CHRM}_tmp/${SAMPLE}.fasta
          
    # Add sample name to header
    #sed -i "1s/^>.*/>${SAMPLE_NAME}/" ${OUTDIR}/${GENE}_tmp/tmp.${GENE}.${SAMPLE_NAME}.fasta
done
}
#=====================================================================================================

function main {

WORKDIR=$SCRATCH/Target_capture #your working directory for this exercise  
METADATA=$HOME/projects/rbge/atam/Target_Capture_tutorial/metadata.csv
BAITS=$WORKDIR/CK_GIT/Ref.fna
REF=$WORKDIR/References #YOU CAN BUILD A REFERENCE DIR IN PROJECT. WE WILL TALK ABOUT THAT LATER


DATA=$WORKDIR/00_DATA
RESULT1=$WORKDIR/01_CLEANED
RESULT2=$WORKDIR/02_MAPPING
RESULT3=$WORKDIR/03_VCF
RESULT4=$WORKDIR/04_EXTRACTS


#setup 
function setup_dir {

mkdir $WORKDIR/CK_GIT -p
mkdir $REF -p
mkdir $DATA -p
mkdir $RESULT1 -p
mkdir $RESULT2 -p
}

setup_dir
#===========================================================================================
#EXECUTE 

#setup_conda
#download_ena $DATA
#GET REFERENCE 
#git clone https://github.com/ckidner/Targeted_enrichment.git $WORKDIR/CK_GIT


#ANALYSIS

#SAMPLE ARRAY: 
SLURM_ARRAY_TASK_ID=2
SAMPLE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" ${METADATA} | cut -d ',' -f1)
echo $SAMPLE

#STEP 1: CLEANING UP READS AND QC REPORT; SAMPLE ARRAY
fastp_qc $DATA $RESULT1 $SAMPLE

#STEP 2: MAPPING: SAMPLE ARRAY
#make index
#bwa index $BAITS
#bwa_map $RESULT1 $BAITS $RESULT2 $SAMPLE

#STEP 3: SNP CALLING: SAMPLE ARRAY
#snp_calling $RESULT2 $RESULT3 $BAITS $SAMPLE
#bcftools_filter $RESULT3 $RESULT3 $SAMPLE

#STEP 4: CONSENSUS; SAMPLE ARRAY
#extract_consensus  $RESULT3 $RESULT4 $BAITS $SAMPLE

#===========================================================================================
#GENE ARRAY
GENE=$(grep '>' $BAITS|sed -n "${SLURM_ARRAY_TASK_ID}p"|cut -d '>' -f 2) #get the Nth gene in the bait set (fasta), remove > from heading
echo $GENE

#STEP 5: ALIGNMENT; GENE ARRAY
#COMPILE FIRST

#STEP 6: PHYLOGENY: GENE ARRAY
}

main


</pre>
</div>


<h2>üß™ Core Functions: Parameter Deep Dive</h2>
<p>We will focus on the <code>fastp_qc</code> function, which performs read trimming and quality control. The parameters are chosen to reflect modern industry standards for Illumina data, specifically optimized for target capture libraries.</p>

<h3>1. Environment & Data Preparation</h3>
<div class="code-block">
#!/bin/bash
#SBATCH --job-name=target_array
#SBATCH --array=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=4G
#SBATCH --output=slurm-%A_%a.out

function setup_conda {
    conda create --name target_capture
    conda install --name target_capture -y fastp bwa samtools bcftools iqtree seqkit blast
}

function download_ena {

OUTDIR=$1

wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776786/ERR776786_1.fastq.gz
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776786/ERR776786_2.fastq.gz 
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776790/ERR776790_1.fastq.gz
wget -nc -P $OUTDIR ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR776/ERR776790/ERR776790_2.fastq.gz
}
</div>
<div class="tip">
    <strong>üí° Key Point:</strong> The <code>--array=1</code> directive sets up a SLURM job array. This allows you to process multiple samples in parallel. The <code>-nc</code> flag in <code>wget</code> prevents re-downloading existing files.
</div>

<h3>2. Read Trimming & QC: <code>fastp_qc</code></h3>
<p>This function replaces legacy tools like Trimmomatic. The parameters below are selected for a balance between stringency and data retention, crucial for target capture where reads are short.</p>

<div class="code-block">
function fastp_qc {
    INDIR=$1
    OUTDIR=$2
    SAMPLE=$3

    fastp -i ${INDIR}/${SAMPLE}_1.fastq.gz -I ${INDIR}/${SAMPLE}_2.fastq.gz \
          -o ${OUTDIR}/${SAMPLE}_R1.fq.gz   -O ${OUTDIR}/${SAMPLE}_R2.fq.gz \
          --unpaired1 ${OUTDIR}/${SAMPLE}_unpaired1.fq.gz \
          --unpaired2 ${OUTDIR}/${SAMPLE}_unpaired2.fq.gz \
          -h ${OUTDIR}/${SAMPLE}.html \
          --json ${OUTDIR}/${SAMPLE}.json \
          --detect_adapter_for_pe \
          --qualified_quality_phred 20 \
          --unqualified_percent_limit 40 \
          -c \
          --cut_front \
          --cut_front_mean_quality 20 \
          --cut_tail \
          --cut_tail_mean_quality 20 \
          --cut_right \
          --cut_right_window_size 4 \
          --cut_right_mean_quality 20 \
          --length_required 36 \
          --trim_poly_g \
          --thread 8 \
          > ${OUTDIR}/${SAMPLE}.fastp.log
}
</div>

<h4>üìä Parameter Justification Table</h4>
<table>
    <thead>
        <tr>
            <th>fastp Parameter</th>
            <th>Value</th>
            <th>Trimmomatic Equivalent</th>
            <th>Rationale for Target Capture</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>--detect_adapter_for_pe</code></td>
            <td>N/A</td>
            <td><code>ILLUMINACLIP</code></td>
            <td>Automatically detects and trims common Illumina adapter sequences. Simpler than providing an explicit FASTA file.</td>
        </tr>
        <tr>
            <td><code>--qualified_quality_phred</code></td>
            <td>20</td>
            <td>Part of <code>SLIDINGWINDOW</code></td>
            <td>Sets a base quality threshold of Q20 (99% accuracy). Bases below this are considered low quality.</td>
        </tr>
        <tr>
            <td><code>--unqualified_percent_limit</code></td>
            <td>40</td>
            <td>Part of <code>SLIDINGWINDOW</code></td>
            <td>Allows up to 40% of bases in a read to be low-quality before discarding. This is permissive, preserving short target fragments.</td>
        </tr>
        <tr>
            <td><code>-c (--correction)</code></td>
            <td>N/A</td>
            <td>None</td>
            <td>Performs base correction in overlapping regions of paired-end reads, reducing sequencing errors.</td>
        </tr>
        <tr>
            <td><code>--cut_front/--cut_tail</code></td>
            <td>Mean Q 20</td>
            <td><code>LEADING:3 / TRAILING:3</code></td>
            <td><span class="param-highlight">Modern Upgrade:</span> Trims low-quality bases (Q<20) from the 5' and 3' ends. The old Q3 threshold is obsolete for modern data.</td>
        </tr>
        <tr>
            <td><code>--cut_right</code></td>
            <td>Window:4, Mean Q:20</td>
            <td><code>SLIDINGWINDOW:4:15</code></td>
            <td><span class="param-highlight">Modern Upgrade:</span> Uses a 4bp sliding window from the 5' end. If the mean quality drops below Q20, the read is truncated. The quality threshold is raised from 15 to 20 for higher confidence.</td>
        </tr>
        <tr>
            <td><code>--length_required</code></td>
            <td>36</td>
            <td><code>MINLEN:36</code></td>
            <td>Discards any read shorter than 36bp after trimming. This is a safe minimum for reliable mapping.</td>
        </tr>
        <tr>
            <td><code>--trim_poly_g</code></td>
            <td>N/A</td>
            <td>None</td>
            <td>Essential for NovaSeq data. Removes poly-G tails caused by two-color chemistry, which appear as high-quality Gs but are artifacts.</td>
        </tr>
    </tbody>
</table>

<div class="important">
    <strong>üî¨ Why Q20 instead of Q3 or Q15?</strong>
    <ul>
        <li><strong>Q3</strong> (from old Trimmomatic tutorials) is essentially no trimming. Modern Illumina reads rarely have bases below Q3.</li>
        <li><strong>Q15</strong> (99.97% accuracy) is a common legacy threshold.</li>
        <li><strong>Q20</strong> (99% accuracy) is the current best practice. It removes genuinely poor-quality bases while retaining the high-quality data typical of modern sequencers. This is especially important for variant calling downstream.</li>
    </ul>
</div>

<div class="important">
    <strong>üî¨ Another good thing about using a script </strong>
    <p><pre>
    you can easily turn your script to paper-styled method section with the help of AI. 
    All you need to do is to copy and paste the script into an AI assistant like ChatGPT or Claude. </pre>
    Raw sequencing reads were subjected to comprehensive quality control using fastp (v0.23.4) with the following parameters. 
    Adapter sequences were automatically detected and removed for paired-end reads (--detect_adapter_for_pe). 
    Reads were filtered based on a minimum Phred score of 20 (--qualified_quality_phred 20), 
    with a maximum allowance of 40% unqualified bases per read (--unqualified_percent_limit 40).  
    A multi-pass trimming strategy was employed to ensure high base quality across read lengths. 
    Bases at the front and tail ends were trimmed if the mean quality fell below 20 (--cut_front, --cut_tail, --cut_front_mean_quality 20). 
    A sliding window scan from the 5' to 3' end trimmed regions where the mean quality in a 4-base window dropped below 20 
    (--cut_right, --cut_right_window_size 4, --cut_right_mean_quality 20). 
    Poly-G tails were removed (--trim_poly_g). 
    Reads shorter than 36 bases after trimming were discarded (--length_required 36). 
    Base correction in overlapped regions of paired-end reads was enabled (-c) to improve consensus accuracy.
   </p>
</div>

<h3>3. Mapping: <code>bwa_map</code> vs <code>bowtie2_map</code></h3>
<p>Both functions are provided, but BWA-MEM is recommended for its sensitivity with indels and its status as the standard for variant calling.</p>
<div class="code-block">
function bwa_map {
    INDIR=$1; REF=$2; OUTDIR=$3; SAMPLE=$4
    minQ=10
    bwa mem $REF ${INDIR}/${SAMPLE}_R1.fq.gz ${INDIR}/${SAMPLE}_R2.fq.gz -t 8 -k 20 |\
        samtools view -Sbh -F 4 -@ 8 -o $OUTDIR/${SAMPLE}.all.bam
    samtools view $OUTDIR/${SAMPLE}.all.bam -Sbh -F 4 -f 3 -q $minQ -@ 8 |\
        samtools sort -@ 8 -o $OUTDIR/${SAMPLE}.sorted.bam
    samtools index $OUTDIR/${SAMPLE}.sorted.bam
}
</div>
<div class="tip">
    <strong>üß¨ Parameter Notes:</strong>
    <ul>
        <li><code>-k 20</code>: Minimum seed length. A lower value (default is 19) increases sensitivity for short target capture reads.</li>
        <li><code>-q $minQ</code> (set to 10): Filters out low-quality alignments after mapping. Q10 is a lenient threshold to keep more data; you can increase this (e.g., Q20) for more stringent analysis.</li>
        <li><code>-f 3</code>: Keeps only reads that are properly paired (0x3 flag).</li>
    </ul>
</div>

<h3>4. Variant Calling & Filtering</h3>
<div class="code-block">
function snp_calling {
    INDIR=$1; OUTDIR=$2; REF=$3; SAMPLE=$4
    minQ=20; minDP=10
    bcftools mpileup -Ou -f $REF $INDIR/${SAMPLE}.sorted.bam --threads 8 \
        --annotate INFO/AD,FORMAT/DP,FORMAT/AD |\
        bcftools call -Ou -mv |\
        bcftools filter -s LowQual -e "QUAL<${minQ} || INFO/DP<${minDP}" \
        > $OUTDIR/${SAMPLE}.flt1.vcf
}

function bcftools_filter {
    INDIR=$1; OUTDIR=$2; SAMPLE=$3
    bcftools view $INDIR/${SAMPLE}.flt1.vcf -O u 2>/dev/null |\
    bcftools +fill-tags -- -t AC,AN,AF,MAF,HWE |\
    bcftools filter -e "QUAL<30" |\
    bcftools filter -e 'INFO/MAF[0] < 0.01' |\
    bcftools filter -e 'INFO/MQ < 30' |\
    bcftools filter -e 'INFO/VDB < 0.1' -Oz -o $OUTDIR/${SAMPLE}.flt3.vcf.gz
    bcftools index $OUTDIR/${SAMPLE}.flt3.vcf.gz -f
}
</div>
<div class="important">
    <strong>‚ö†Ô∏è Filtering Logic:</strong>
    <ul>
        <li><code>minQ=20</code>: Initial hard filter on raw QUAL score.</li>
        <li><code>minDP=10</code>: Minimum read depth at a site.</li>
        <li><code>MAF[0] < 0.01</code>: Removes rare variants (Minor Allele Frequency < 1%).</li>
        <li><code>MQ < 30</code>: Removes sites with low mapping quality.</li>
        <li><code>VDB < 0.1</code>: Filters based on Variant Distance Bias, removing likely false positives.</li>
    </ul>
</div>

<h2>üöÄ Running the Pipeline (The <code>main</code> Function)</h2>
<p>The <code>main</code> function orchestrates the workflow. It sets up directories, defines key paths, and executes the analysis functions. Note the use of <code>SLURM_ARRAY_TASK_ID</code> to process samples and genes in parallel.</p>

<div class="code-block">
function main {
    WORKDIR=$SCRATCH/Target_capture
    METADATA=$HOME/.../metadata.csv
    BAITS=$WORKDIR/CK_GIT/Ref.fna
    DATA=$WORKDIR/00_DATA; RESULT1=$WORKDIR/01_CLEANED; RESULT2=$WORKDIR/02_MAPPING
    RESULT3=$WORKDIR/03_VCF; RESULT4=$WORKDIR/04_EXTRACTS

    function setup_dir { mkdir -p $WORKDIR/CK_GIT $REF $DATA $RESULT1 $RESULT2 $RESULT3 $RESULT4; }
    setup_dir

    # Download reference (uncomment to run)
    # git clone https://github.com/ckidner/Targeted_enrichment.git $WORKDIR/CK_GIT

    # Sample-level processing (SLURM array task ID picks the sample)
    SLURM_ARRAY_TASK_ID=2  # In a real job, this is set by SLURM
    SAMPLE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" ${METADATA} | cut -d ',' -f1)
    echo "Processing sample: $SAMPLE"

    fastp_qc $DATA $RESULT1 $SAMPLE
    # bwa_map $RESULT1 $BAITS $RESULT2 $SAMPLE
    # snp_calling $RESULT2 $RESULT3 $BAITS $SAMPLE
    # bcftools_filter $RESULT3 $RESULT3 $SAMPLE

    # Gene-level processing (for consensus & alignment)
    GENE=$(grep '>' $BAITS | sed -n "${SLURM_ARRAY_TASK_ID}p" | cut -d '>' -f 2)
    echo "Processing gene: $GENE"
    # extract_consensus $RESULT3 $RESULT4 $BAITS $SAMPLE
}

main
</div>

<div class="exercise">
    <h3>‚úçÔ∏è Exercise: Parameter Exploration</h3>
    <p>Run the <code>fastp_qc</code> function on a single sample using the parameters above. Then, modify the <code>--cut_right_mean_quality</code> to 15 and re-run.</p>
    <ol>
        <li>Compare the two HTML reports. How does the read retention rate change?</li>
        <li>Why might a lower quality threshold (Q15) be undesirable for variant calling?</li>
        <li>Look at the "Read1 before filtering" vs "after filtering" quality plots. At which position does the Q20-trimmed read start to drop?</li>
    </ol>
</div>

<h2>üìå Summary & Best Practices</h2>
<ul>
    <li><strong>Modularity:</strong> Functions keep the pipeline organized and debuggable.</li>
    <li><strong>Parameter Selection:</strong> Always question legacy parameters. Q20 is the new Q15 for modern data.</li>
    <li><strong>Target Capture Nuances:</strong> Use <code>--trim_poly_g</code>, be lenient with read length (<code>--length_required 36</code>), and use sensitive mappers (BWA-MEM).</li>
    <li><strong>SLURM Integration:</strong> Array jobs allow scaling to hundreds of samples. Use <code>--array</code> and <code>SLURM_ARRAY_TASK_ID</code> to parallelize effectively.</li>
</ul>

<div class="tip">
    <p>üìö <strong>Further Reading:</strong> Explore the <code>extract_consensus</code> function as a next step. It builds on the VCF outputs to create sample-specific sequences for phylogenetic analysis.</p>
</div>

</body>
</html>